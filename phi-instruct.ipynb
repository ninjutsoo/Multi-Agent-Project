{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Using Multiple GPUs: NVIDIA H100 PCIe, NVIDIA H100 PCIe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hq6375/.conda/envs/temp/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/hq6375/.conda/envs/temp/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa5bd443dea745af9ce056a487a89a47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Mistral-7B-Instruct-v0.3 successfully loaded according to GPU_MODE!\n"
     ]
    }
   ],
   "source": [
    "#cell 1\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# ‚úÖ Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# ‚úÖ Set Hugging Face Token\n",
    "hft = os.getenv(\"HF_TOKEN\")\n",
    "if not hft:\n",
    "    raise ValueError(\"‚ùå HF_TOKEN not found in environment variables! Make sure your .env file exists.\")\n",
    "\n",
    "# ‚úÖ Set Hugging Face Cache Directory\n",
    "os.environ[\"HF_HOME\"] = \"/home/hq6375/huggingface_Amin\"\n",
    "cache_dir = os.environ[\"HF_HOME\"]\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "\n",
    "# ‚úÖ Define GPU Mode (0: GPU 0, 1: GPU 1, 2: Both GPUs)\n",
    "GPU_MODE = 2  # Change this variable only (0, 1, or 2)\n",
    "\n",
    "# ‚úÖ Check CUDA availability\n",
    "if not torch.cuda.is_available():\n",
    "    raise RuntimeError(\"‚ùå No CUDA-compatible GPU found!\")\n",
    "\n",
    "# ‚úÖ Select Device Mapping Based on GPU Mode\n",
    "if GPU_MODE == 0:\n",
    "    device_map = {\"\": 0}  # Load everything on GPU 0\n",
    "    print(f\"‚úÖ Using GPU 0: {torch.cuda.get_device_name(0)}\")\n",
    "elif GPU_MODE == 1:\n",
    "    device_map = {\"\": 1}  # Load everything on GPU 1\n",
    "    print(f\"‚úÖ Using GPU 1: {torch.cuda.get_device_name(1)}\")\n",
    "elif GPU_MODE == 2:\n",
    "    device_map = \"auto\"  # Automatically distribute across GPUs\n",
    "    print(f\"‚úÖ Using Multiple GPUs: {torch.cuda.get_device_name(0)}, {torch.cuda.get_device_name(1)}\")\n",
    "else:\n",
    "    raise ValueError(\"‚ùå Invalid GPU_MODE! Use 0 (GPU 0), 1 (GPU 1), or 2 (both GPUs).\")\n",
    "\n",
    "# ‚úÖ Model ID\n",
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "# model_id = \"meta-llama/Llama-4-Scout-17B-16E-Instruct\"\n",
    "\n",
    "\n",
    "# ‚úÖ Load tokenizer with authentication\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, token=hft, trust_remote_code=True)\n",
    "\n",
    "# ‚úÖ Load model and distribute across GPUs\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    token=hft,\n",
    "    torch_dtype=torch.bfloat16,  # Use bf16 for lower memory usage\n",
    "    low_cpu_mem_usage=True,  # Prevent CPU offloading\n",
    "    trust_remote_code=True,\n",
    "    device_map=device_map  # Automatically maps model to GPU(s)\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Mistral-7B-Instruct-v0.3 successfully loaded according to GPU_MODE!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ CELL 2: Information Extraction from Note\n",
    "MIN_TOKENS = 1000\n",
    "MAX_TOKENS = 4000\n",
    "\n",
    "def extract_information(note, hadm_id=\"UNKNOWN\"):\n",
    "    tokenized_note = tokenizer(note, truncation=False, return_tensors=\"pt\")\n",
    "    note_tokens = len(tokenized_note[\"input_ids\"][0])\n",
    "    if note_tokens > MAX_TOKENS:\n",
    "        print(f\"‚ö† Skipping HADM_ID {hadm_id} (Tokens: {note_tokens})\")\n",
    "        return None\n",
    "\n",
    "    instruction = \"\"\"Extract only the most relevant medical details that may impact 30-day readmission risk from the following discharge summary. Your response must be in **valid JSON** format, short and precise, and follow this exact structure:\n",
    "\n",
    "{\n",
    "    \"past_hospitalizations\": [\"List prior hospital admissions briefly, e.g., 'CHF in 2021', or return [] if none are mentioned.\"],\n",
    "    \"discharge_diagnoses\": [\"List only the main diagnoses at discharge, no explanations or duplication, or return [] if none are stated.\"],\n",
    "    \"high_risk_medications\": [\"List high-risk medications prescribed at discharge (e.g., insulin, opioids, anticoagulants), or return [] if none.\"],\n",
    "    \"follow_up_instructions\": [\"List specific follow-ups (e.g., 'Cardiology in 2 weeks'), or return [] if not clearly given.\"],\n",
    "    \"discharge_disposition\": \"Where the patient is being sent after discharge (e.g., 'home', 'SNF', 'rehab', 'another hospital'), or return \\\"\\\".\",\n",
    "    \"condition_at_discharge\": \"Summarize discharge stability in a few words (e.g., 'stable', 'still symptomatic'), or return \\\"\\\".\"\n",
    "}\n",
    "\n",
    "ONLY output the JSON. Do NOT include explanations, notes, or filler text. Be brief and structured.\n",
    "<|end|>\n",
    "\"\"\"\n",
    "\n",
    "    input_text = f\"{instruction}\\n\\n{note}\"\n",
    "    tokenized_input = tokenizer(input_text, return_tensors=\"pt\", truncation=True, max_length=MAX_TOKENS).to(model.device)\n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **tokenized_input,\n",
    "                max_new_tokens=512,\n",
    "                temperature=0.1,\n",
    "                do_sample=False,\n",
    "                eos_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "        extracted_info = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "        first_brace = extracted_info.find(\"{\")\n",
    "        second_brace = extracted_info.find(\"{\", first_brace + 1)\n",
    "        json_end = extracted_info.rfind(\"}\")\n",
    "        json_start = second_brace if second_brace != -1 else first_brace\n",
    "        extracted_json = extracted_info[json_start:json_end + 1].strip() if json_start != -1 and json_end != -1 and \"List prior hospital admissions briefly\" not in extracted_info[json_start:json_end + 1].strip() else \"ERROR\"\n",
    "        print(f\"\\n‚úÖ HADM_ID {hadm_id} | Extracted Output:\\n{extracted_json}\\n{'-'*80}\")\n",
    "        return extracted_json\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error extracting info from HADM_ID {hadm_id}: {e}\")\n",
    "        return \"ERROR\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ CELL 3: Print function (unchanged)\n",
    "def print_extracted_patient(index=0):\n",
    "    if \"extracted_results\" not in globals() or len(extracted_results) == 0:\n",
    "        print(\"‚ö† No extracted results found. Run the extraction first.\")\n",
    "        return\n",
    "    if index < 0 or index >= len(extracted_results):\n",
    "        print(f\"‚ö† Invalid index {index}.\")\n",
    "        return\n",
    "    patient = extracted_results[index]\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"ü©∫ Patient ID: {patient.get('patient_id', 'Unknown')}\")\n",
    "    print(f\"üìÑ Note Preview:\\n{patient.get('original_notes', '')[:200]}...\")\n",
    "    print(f\"üìù Extracted JSON:\\n{patient.get('extracted_info', '')}\\n\")\n",
    "    print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded cleaned data.\n",
      "üìä Total notes: 52726\n"
     ]
    }
   ],
   "source": [
    "# ‚úÖ CELL 4: Load discharge summaries only\n",
    "import pandas as pd\n",
    "note_path = \"/home/hq6375/Desktop/Code/MIMIC-III/physionet.org/files/mimiciii/1.4/NOTEEVENTS.csv\"\n",
    "admissions_path = \"/home/hq6375/Desktop/Code/MIMIC-III/physionet.org/files/mimiciii/1.4/ADMISSIONS.csv\"\n",
    "cleaned_path = \"/home/hq6375/Desktop/Code/MIMIC-III/cleaned_NOTES.csv\"\n",
    "\n",
    "if os.path.exists(cleaned_path):\n",
    "    df = pd.read_csv(cleaned_path)\n",
    "    print(\"‚úÖ Loaded cleaned data.\")\n",
    "else:\n",
    "    df = pd.read_csv(note_path)\n",
    "    admissions = pd.read_csv(admissions_path)\n",
    "    df = df[df[\"CATEGORY\"] == \"Discharge summary\"]\n",
    "    df = df.sort_values(by=[\"SUBJECT_ID\", \"HADM_ID\", \"CHARTDATE\"])\n",
    "    df = df.groupby([\"SUBJECT_ID\", \"HADM_ID\"]).tail(1)\n",
    "    df = df.dropna(subset=[\"TEXT\"])\n",
    "    df[\"CLEANED_TEXT\"] = df[\"TEXT\"].str.replace(r\"\\n|\\r\", \" \", regex=True).str.lower().str.strip()\n",
    "    df.to_csv(cleaned_path, index=False)\n",
    "    print(\"‚úÖ Cleaned and saved notes.\")\n",
    "print(f\"üìä Total notes: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2010645/601449405.py:4: DtypeWarning: Columns (4,5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(note_path)\n"
     ]
    }
   ],
   "source": [
    "# ‚úÖ CELL 4: Load discharge summaries only\n",
    "import pandas as pd\n",
    "note_path = \"/home/hq6375/Desktop/Code/MIMIC-III/physionet.org/files/mimiciii/1.4/NOTEEVENTS.csv\"\n",
    "df = pd.read_csv(note_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of           ROW_ID  SUBJECT_ID   HADM_ID   CHARTDATE            CHARTTIME  \\\n",
       "0            174       22532  167853.0  2151-08-04                  NaN   \n",
       "1            175       13702  107527.0  2118-06-14                  NaN   \n",
       "2            176       13702  167118.0  2119-05-25                  NaN   \n",
       "3            177       13702  196489.0  2124-08-18                  NaN   \n",
       "4            178       26880  135453.0  2162-03-25                  NaN   \n",
       "...          ...         ...       ...         ...                  ...   \n",
       "2083175  2070657       31097  115637.0  2132-01-21  2132-01-21 03:27:00   \n",
       "2083176  2070658       31097  115637.0  2132-01-21  2132-01-21 09:50:00   \n",
       "2083177  2070659       31097  115637.0  2132-01-21  2132-01-21 16:42:00   \n",
       "2083178  2070660       31097  115637.0  2132-01-21  2132-01-21 18:05:00   \n",
       "2083179  2070661       31097  115637.0  2132-01-21  2132-01-21 18:05:00   \n",
       "\n",
       "                   STORETIME           CATEGORY DESCRIPTION     CGID  ISERROR  \\\n",
       "0                        NaN  Discharge summary      Report      NaN      NaN   \n",
       "1                        NaN  Discharge summary      Report      NaN      NaN   \n",
       "2                        NaN  Discharge summary      Report      NaN      NaN   \n",
       "3                        NaN  Discharge summary      Report      NaN      NaN   \n",
       "4                        NaN  Discharge summary      Report      NaN      NaN   \n",
       "...                      ...                ...         ...      ...      ...   \n",
       "2083175  2132-01-21 03:38:00      Nursing/other      Report  17581.0      NaN   \n",
       "2083176  2132-01-21 09:53:00      Nursing/other      Report  19211.0      NaN   \n",
       "2083177  2132-01-21 16:44:00      Nursing/other      Report  20104.0      NaN   \n",
       "2083178  2132-01-21 18:16:00      Nursing/other      Report  16023.0      NaN   \n",
       "2083179  2132-01-21 18:31:00      Nursing/other      Report  16023.0      NaN   \n",
       "\n",
       "                                                      TEXT  \n",
       "0        Admission Date:  [**2151-7-16**]       Dischar...  \n",
       "1        Admission Date:  [**2118-6-2**]       Discharg...  \n",
       "2        Admission Date:  [**2119-5-4**]              D...  \n",
       "3        Admission Date:  [**2124-7-21**]              ...  \n",
       "4        Admission Date:  [**2162-3-3**]              D...  \n",
       "...                                                    ...  \n",
       "2083175  NPN\\n\\n\\n#1  Infant remains in RA with O2 sats...  \n",
       "2083176  Neonatology\\nDOL #5, CGA 36 weeks.\\n\\nCVR: Con...  \n",
       "2083177  Family Meeting Note\\nFamily meeting held with ...  \n",
       "2083178  NPN 1800\\n\\n\\n#1 Resp: [**Known lastname 2243*...  \n",
       "2083179  NPN 1800\\nNursing Addendum:\\n[**Known lastname...  \n",
       "\n",
       "[2083180 rows x 11 columns]>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Labels saved.\n"
     ]
    }
   ],
   "source": [
    "# ‚úÖ CELL 5: Generate readmission labels\n",
    "admissions = pd.read_csv(admissions_path)\n",
    "admissions[\"ADMITTIME\"] = pd.to_datetime(admissions[\"ADMITTIME\"])\n",
    "admissions[\"DISCHTIME\"] = pd.to_datetime(admissions[\"DISCHTIME\"])\n",
    "admissions[\"DEATHTIME\"] = pd.to_datetime(admissions[\"DEATHTIME\"])\n",
    "admissions = admissions.sort_values(by=[\"SUBJECT_ID\", \"ADMITTIME\"])\n",
    "admissions[\"NEXT_ADMITTIME\"] = admissions.groupby(\"SUBJECT_ID\")[\"ADMITTIME\"].shift(-1)\n",
    "admissions[\"NEXT_ADMISSION_TYPE\"] = admissions.groupby(\"SUBJECT_ID\")[\"ADMISSION_TYPE\"].shift(-1)\n",
    "admissions.loc[admissions[\"NEXT_ADMISSION_TYPE\"] == \"ELECTIVE\", \"NEXT_ADMITTIME\"] = pd.NaT\n",
    "admissions = admissions[admissions[\"DEATHTIME\"].isna()]\n",
    "admissions = admissions[admissions[\"ADMISSION_TYPE\"] != \"NEWBORN\"]\n",
    "admissions[\"readmission_30d\"] = ((admissions[\"NEXT_ADMITTIME\"] - admissions[\"DISCHTIME\"]).dt.total_seconds() < 30 * 86400).astype(int)\n",
    "readmission_labels = admissions[[\"SUBJECT_ID\", \"HADM_ID\", \"readmission_30d\"]]\n",
    "readmission_labels.to_csv(\"readmission_labels.csv\", index=False)\n",
    "print(\"‚úÖ Labels saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Notes with readmission labels: 43880\n"
     ]
    }
   ],
   "source": [
    "# ‚úÖ CELL 6: Merge labels into notes\n",
    "df = df.merge(readmission_labels, on=[\"SUBJECT_ID\", \"HADM_ID\"], how=\"inner\")\n",
    "print(f\"‚úÖ Notes with readmission labels: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ CELL 7: Controlled extraction ‚Äî 300 negative, 100 positive only\n",
    "import torch.nn.functional as F\n",
    "import json\n",
    "\n",
    "# ‚úÖ Token estimation for filtering\n",
    "df[\"CLEANED_TEXT\"] = df[\"CLEANED_TEXT\"].astype(str)\n",
    "df[\"TOKEN_ESTIMATE\"] = df[\"CLEANED_TEXT\"].apply(lambda x: len(x) // 4)\n",
    "filtered_df = df[(df[\"TOKEN_ESTIMATE\"] >= MIN_TOKENS) & (df[\"TOKEN_ESTIMATE\"] <= MAX_TOKENS)].copy()\n",
    "\n",
    "# ‚úÖ Output file path\n",
    "save_path = \"./batch_extractions_only.csv\"\n",
    "if not os.path.exists(save_path):\n",
    "    pd.DataFrame(columns=[\n",
    "        \"HADM_ID\", \"CLEANED_TEXT\", \"extracted_json\", \"readmission_30d\"\n",
    "        # \"predicted_token\", \"prob_yes\", \"prob_no\", \"prob_yes_normalized\", \"top_5_tokens\"  # <- prediction columns commented\n",
    "    ]).to_csv(save_path, index=False)\n",
    "\n",
    "# ‚úÖ Initialize tracking\n",
    "results = []\n",
    "negatives_saved = 0\n",
    "positives_saved = 0\n",
    "MAX_NEG = 300\n",
    "MAX_POS = 100\n",
    "attempts = 0\n",
    "max_attempts = 2000  # Allow more tries\n",
    "\n",
    "# ‚úÖ Split by class for targeted sampling\n",
    "neg_df = filtered_df[filtered_df[\"readmission_30d\"] == 0].copy()\n",
    "pos_df = filtered_df[filtered_df[\"readmission_30d\"] == 1].copy()\n",
    "\n",
    "while (negatives_saved < MAX_NEG or positives_saved < MAX_POS) and attempts < max_attempts:\n",
    "    if (neg_df.empty and negatives_saved < MAX_NEG) or (pos_df.empty and positives_saved < MAX_POS):\n",
    "        print(\"‚ùå Not enough notes available to meet target counts.\")\n",
    "        break\n",
    "\n",
    "    if negatives_saved < MAX_NEG:\n",
    "        row = neg_df.sample(1)\n",
    "        label = 0\n",
    "    else:\n",
    "        row = pos_df.sample(1)\n",
    "        label = 1\n",
    "\n",
    "    hadm_id = row[\"HADM_ID\"].values[0]\n",
    "    text = row[\"CLEANED_TEXT\"].values[0]\n",
    "\n",
    "    # Remove sampled row from corresponding pool\n",
    "    if label == 0:\n",
    "        neg_df = neg_df[neg_df[\"HADM_ID\"] != hadm_id]\n",
    "    else:\n",
    "        pos_df = pos_df[pos_df[\"HADM_ID\"] != hadm_id]\n",
    "\n",
    "    extracted_json = extract_information(text, hadm_id=hadm_id)\n",
    "    if not extracted_json or not extracted_json.startswith(\"{\") or \"List prior hospital admissions briefly\" in extracted_json:\n",
    "        print(f\"‚ö† Skipping HADM_ID {hadm_id} due to invalid extraction.\")\n",
    "        attempts += 1\n",
    "        continue\n",
    "\n",
    "    row_data = {\n",
    "        \"HADM_ID\": hadm_id,\n",
    "        \"CLEANED_TEXT\": text,\n",
    "        \"extracted_json\": extracted_json,\n",
    "        \"readmission_30d\": label,\n",
    "        # \"predicted_token\": None,\n",
    "        # \"prob_yes\": None,\n",
    "        # \"prob_no\": None,\n",
    "        # \"prob_yes_normalized\": None,\n",
    "        # \"top_5_tokens\": None\n",
    "    }\n",
    "\n",
    "    pd.DataFrame([row_data]).to_csv(save_path, mode=\"a\", header=False, index=False)\n",
    "    results.append(row_data)\n",
    "\n",
    "    if label == 0:\n",
    "        negatives_saved += 1\n",
    "    else:\n",
    "        positives_saved += 1\n",
    "\n",
    "    print(f\"‚úÖ Total: {len(results)} | Pos: {positives_saved}/100 | Neg: {negatives_saved}/300 (HADM_ID: {hadm_id})\")\n",
    "    attempts += 1\n",
    "\n",
    "# ‚úÖ Uncomment the block below to re-enable prediction\n",
    "# instruction = (\n",
    "#     \"Based on the following patient data, will the patient be readmitted within 30 days?\\n\"\n",
    "#     \"You must answer using **only one token**, either 'yes' or 'no'.\\n\"\n",
    "# )\n",
    "# llm_input = f\"{instruction}\\n{extracted_json}\\n\\nAnswer:\"\n",
    "# tokenized_input = tokenizer(llm_input, return_tensors=\"pt\", truncation=True, max_length=MAX_TOKENS).to(model.device)\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     logits = model(**tokenized_input).logits\n",
    "#     last_logits = logits[0, -1]\n",
    "#     probs = F.softmax(last_logits, dim=-1)\n",
    "#     yes_token_id = tokenizer(\"yes\", add_special_tokens=False).input_ids[0]\n",
    "#     no_token_id = tokenizer(\"no\", add_special_tokens=False).input_ids[0]\n",
    "#     prob_yes = probs[yes_token_id].item()\n",
    "#     prob_no = probs[no_token_id].item()\n",
    "#     prob_yes_norm = prob_yes / (prob_yes + prob_no)\n",
    "#     topk = torch.topk(probs, k=5)\n",
    "#     top_tokens = [(tokenizer.decode([i]).strip(), round(p.item(), 5)) for i, p in zip(topk.indices, topk.values)]\n",
    "\n",
    "#     row_data.update({\n",
    "#         \"predicted_token\": \"yes\" if prob_yes > prob_no else \"no\",\n",
    "#         \"prob_yes\": prob_yes,\n",
    "#         \"prob_no\": prob_no,\n",
    "#         \"prob_yes_normalized\": prob_yes_norm,\n",
    "#         \"top_5_tokens\": json.dumps(top_tokens)\n",
    "#     })\n",
    "\n",
    "# ‚úÖ Final status message\n",
    "if positives_saved < MAX_POS or negatives_saved < MAX_NEG:\n",
    "    print(f\"‚ö† Only {positives_saved} positives and {negatives_saved} negatives extracted.\")\n",
    "else:\n",
    "    print(\"üéâ Done! Extracted exactly 100 positives and 300 negatives.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "temp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
